---
title: 'd-VMP: Distributed Variational Message Passing'
abstract: Motivated by a real-world financial dataset, we propose a distributed variational
  message passing scheme for learning conjugate exponential models. We show that the
  method can be seen as a projected natural gradient ascent algorithm, and it therefore
  has good convergence properties. This is supported experimentally, where we show
  that the approach is robust wrt. common problems like imbalanced data, heavy-tailed
  empirical distributions, and a high degree of missing values. The scheme is based
  on map-reduce operations, and utilizes the memory management of modern big data
  frameworks like Apache Flink to obtain a time-efficient and scalable implementation.
  The proposed algorithm compares favourably to stochastic variational inference both
  in terms of speed and quality of the learned models. For the scalability analysis,
  we evaluate our approach over a network with more than one billion nodes (and approx.
  75% latent variables) using a computer cluster with 128 processing units.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: masegosa16
month: 0
tex_title: 'd-{VMP}: Distributed Variational Message Passing'
firstpage: 321
lastpage: 332
page: 321-332
sections: 
author:
- given: Andrés R.
  family: Masegosa
- given: Ana M.
  family: Martı́nez
- given: Helge
  family: Langseth
- given: Thomas D.
  family: Nielsen
- given: Antonio
  family: Salmerón
- given: Darío
  family: Ramos-López
- given: Anders L.
  family: Madsen
date: 2016-08-15
address: Lugano, Switzerland
publisher: PMLR
container-title: Proceedings of the Eighth International Conference on Probabilistic
  Graphical Models
volume: '52'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 8
  - 15
pdf: http://proceedings.mlr.press/v52/masegosa16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
